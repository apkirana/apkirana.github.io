<html>

<head>
<meta http-equiv=Content-Type content="text/html; charset=utf-8">
<meta name=Generator content="Microsoft Word 15 (filtered)">
<style>
 /* Font Definitions */
 @font-face
	{font-family:Wingdings;
	panose-1:5 0 0 0 0 0 0 0 0 0;}
@font-face
	{font-family:"Cambria Math";
	panose-1:2 4 5 3 5 4 6 3 2 4;}
@font-face
	{font-family:"Aptos Display";
	panose-1:2 11 0 4 2 2 2 2 2 4;}
@font-face
	{font-family:Aptos;
	panose-1:2 11 0 4 2 2 2 2 2 4;}
 /* Style Definitions */
 p.MsoNormal, li.MsoNormal, div.MsoNormal
	{margin-top:0in;
	margin-right:0in;
	margin-bottom:8.0pt;
	margin-left:0in;
	line-height:115%;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;}
h1
	{mso-style-link:"Heading 1 Char";
	margin-top:.25in;
	margin-right:0in;
	margin-bottom:4.0pt;
	margin-left:0in;
	line-height:115%;
	page-break-after:avoid;
	font-size:20.0pt;
	font-family:"Aptos Display",sans-serif;
	color:#BF4E14;}
h2
	{mso-style-link:"Heading 2 Char";
	margin-top:8.0pt;
	margin-right:0in;
	margin-bottom:4.0pt;
	margin-left:0in;
	line-height:115%;
	page-break-after:avoid;
	font-size:16.0pt;
	font-family:"Aptos Display",sans-serif;
	color:#BF4E14;}
h3
	{mso-style-link:"Heading 3 Char";
	margin-top:8.0pt;
	margin-right:0in;
	margin-bottom:4.0pt;
	margin-left:0in;
	line-height:115%;
	page-break-after:avoid;
	font-size:14.0pt;
	font-family:"Aptos",sans-serif;
	color:#E97132;}
h4
	{mso-style-link:"Heading 4 Char";
	margin-top:4.0pt;
	margin-right:0in;
	margin-bottom:2.0pt;
	margin-left:0in;
	line-height:115%;
	page-break-after:avoid;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;
	color:#E97132;
	font-weight:normal;
	font-style:italic;}
h5
	{mso-style-link:"Heading 5 Char";
	margin-top:4.0pt;
	margin-right:0in;
	margin-bottom:2.0pt;
	margin-left:0in;
	line-height:115%;
	page-break-after:avoid;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;
	color:#0F4761;
	font-weight:normal;}
h6
	{mso-style-link:"Heading 6 Char";
	margin-top:2.0pt;
	margin-right:0in;
	margin-bottom:0in;
	margin-left:0in;
	line-height:115%;
	page-break-after:avoid;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;
	color:#595959;
	font-weight:normal;
	font-style:italic;}
p.MsoHeading7, li.MsoHeading7, div.MsoHeading7
	{mso-style-link:"Heading 7 Char";
	margin-top:2.0pt;
	margin-right:0in;
	margin-bottom:0in;
	margin-left:0in;
	line-height:115%;
	page-break-after:avoid;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;
	color:#595959;}
p.MsoHeading8, li.MsoHeading8, div.MsoHeading8
	{mso-style-link:"Heading 8 Char";
	margin:0in;
	line-height:115%;
	page-break-after:avoid;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;
	color:#272727;
	font-style:italic;}
p.MsoHeading9, li.MsoHeading9, div.MsoHeading9
	{mso-style-link:"Heading 9 Char";
	margin:0in;
	line-height:115%;
	page-break-after:avoid;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;
	color:#272727;}
p.MsoToc1, li.MsoToc1, div.MsoToc1
	{margin-top:6.0pt;
	margin-right:0in;
	margin-bottom:0in;
	margin-left:0in;
	line-height:115%;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;
	font-weight:bold;
	font-style:italic;}
p.MsoToc2, li.MsoToc2, div.MsoToc2
	{margin-top:6.0pt;
	margin-right:0in;
	margin-bottom:0in;
	margin-left:12.0pt;
	line-height:115%;
	font-size:11.0pt;
	font-family:"Aptos",sans-serif;
	font-weight:bold;}
p.MsoToc3, li.MsoToc3, div.MsoToc3
	{margin-top:0in;
	margin-right:0in;
	margin-bottom:0in;
	margin-left:24.0pt;
	line-height:115%;
	font-size:10.0pt;
	font-family:"Aptos",sans-serif;}
p.MsoToc4, li.MsoToc4, div.MsoToc4
	{margin-top:0in;
	margin-right:0in;
	margin-bottom:0in;
	margin-left:.5in;
	line-height:115%;
	font-size:10.0pt;
	font-family:"Aptos",sans-serif;}
p.MsoToc5, li.MsoToc5, div.MsoToc5
	{margin-top:0in;
	margin-right:0in;
	margin-bottom:0in;
	margin-left:48.0pt;
	line-height:115%;
	font-size:10.0pt;
	font-family:"Aptos",sans-serif;}
p.MsoToc6, li.MsoToc6, div.MsoToc6
	{margin-top:0in;
	margin-right:0in;
	margin-bottom:0in;
	margin-left:60.0pt;
	line-height:115%;
	font-size:10.0pt;
	font-family:"Aptos",sans-serif;}
p.MsoToc7, li.MsoToc7, div.MsoToc7
	{margin-top:0in;
	margin-right:0in;
	margin-bottom:0in;
	margin-left:1.0in;
	line-height:115%;
	font-size:10.0pt;
	font-family:"Aptos",sans-serif;}
p.MsoToc8, li.MsoToc8, div.MsoToc8
	{margin-top:0in;
	margin-right:0in;
	margin-bottom:0in;
	margin-left:84.0pt;
	line-height:115%;
	font-size:10.0pt;
	font-family:"Aptos",sans-serif;}
p.MsoToc9, li.MsoToc9, div.MsoToc9
	{margin-top:0in;
	margin-right:0in;
	margin-bottom:0in;
	margin-left:96.0pt;
	line-height:115%;
	font-size:10.0pt;
	font-family:"Aptos",sans-serif;}
p.MsoTitle, li.MsoTitle, div.MsoTitle
	{mso-style-link:"Title Char";
	margin-top:0in;
	margin-right:0in;
	margin-bottom:4.0pt;
	margin-left:0in;
	font-size:28.0pt;
	font-family:"Aptos Display",sans-serif;
	letter-spacing:-.5pt;}
p.MsoTitleCxSpFirst, li.MsoTitleCxSpFirst, div.MsoTitleCxSpFirst
	{mso-style-link:"Title Char";
	margin:0in;
	font-size:28.0pt;
	font-family:"Aptos Display",sans-serif;
	letter-spacing:-.5pt;}
p.MsoTitleCxSpMiddle, li.MsoTitleCxSpMiddle, div.MsoTitleCxSpMiddle
	{mso-style-link:"Title Char";
	margin:0in;
	font-size:28.0pt;
	font-family:"Aptos Display",sans-serif;
	letter-spacing:-.5pt;}
p.MsoTitleCxSpLast, li.MsoTitleCxSpLast, div.MsoTitleCxSpLast
	{mso-style-link:"Title Char";
	margin-top:0in;
	margin-right:0in;
	margin-bottom:4.0pt;
	margin-left:0in;
	font-size:28.0pt;
	font-family:"Aptos Display",sans-serif;
	letter-spacing:-.5pt;}
p.MsoSubtitle, li.MsoSubtitle, div.MsoSubtitle
	{mso-style-link:"Subtitle Char";
	margin-top:0in;
	margin-right:0in;
	margin-bottom:8.0pt;
	margin-left:0in;
	line-height:115%;
	font-size:14.0pt;
	font-family:"Aptos",sans-serif;
	color:#595959;
	letter-spacing:.75pt;}
a:link, span.MsoHyperlink
	{color:#467886;
	text-decoration:underline;}
a:visited, span.MsoHyperlinkFollowed
	{color:#96607D;
	text-decoration:underline;}
p.MsoListParagraph, li.MsoListParagraph, div.MsoListParagraph
	{margin-top:0in;
	margin-right:0in;
	margin-bottom:8.0pt;
	margin-left:.5in;
	line-height:115%;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;}
p.MsoListParagraphCxSpFirst, li.MsoListParagraphCxSpFirst, div.MsoListParagraphCxSpFirst
	{margin-top:0in;
	margin-right:0in;
	margin-bottom:0in;
	margin-left:.5in;
	line-height:115%;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;}
p.MsoListParagraphCxSpMiddle, li.MsoListParagraphCxSpMiddle, div.MsoListParagraphCxSpMiddle
	{margin-top:0in;
	margin-right:0in;
	margin-bottom:0in;
	margin-left:.5in;
	line-height:115%;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;}
p.MsoListParagraphCxSpLast, li.MsoListParagraphCxSpLast, div.MsoListParagraphCxSpLast
	{margin-top:0in;
	margin-right:0in;
	margin-bottom:8.0pt;
	margin-left:.5in;
	line-height:115%;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;}
p.MsoQuote, li.MsoQuote, div.MsoQuote
	{mso-style-link:"Quote Char";
	margin-top:8.0pt;
	margin-right:0in;
	margin-bottom:8.0pt;
	margin-left:0in;
	text-align:center;
	line-height:115%;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;
	color:#404040;
	font-style:italic;}
p.MsoIntenseQuote, li.MsoIntenseQuote, div.MsoIntenseQuote
	{mso-style-link:"Intense Quote Char";
	margin-top:.25in;
	margin-right:.6in;
	margin-bottom:.25in;
	margin-left:.6in;
	text-align:center;
	line-height:115%;
	border:none;
	padding:0in;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;
	color:#0F4761;
	font-style:italic;}
span.MsoIntenseEmphasis
	{color:#0F4761;
	font-style:italic;}
span.MsoIntenseReference
	{font-variant:small-caps;
	color:#0F4761;
	letter-spacing:.25pt;
	font-weight:bold;}
p.MsoTocHeading, li.MsoTocHeading, div.MsoTocHeading
	{margin-top:24.0pt;
	margin-right:0in;
	margin-bottom:0in;
	margin-left:0in;
	line-height:115%;
	page-break-after:avoid;
	font-size:14.0pt;
	font-family:"Aptos Display",sans-serif;
	color:#BF4E14;}
span.Heading1Char
	{mso-style-name:"Heading 1 Char";
	mso-style-link:"Heading 1";
	font-family:"Aptos Display",sans-serif;
	color:#BF4E14;
	font-weight:bold;}
span.Heading2Char
	{mso-style-name:"Heading 2 Char";
	mso-style-link:"Heading 2";
	font-family:"Aptos Display",sans-serif;
	color:#BF4E14;
	font-weight:bold;}
span.Heading3Char
	{mso-style-name:"Heading 3 Char";
	mso-style-link:"Heading 3";
	font-family:"Times New Roman",serif;
	color:#E97132;
	font-weight:bold;}
span.Heading4Char
	{mso-style-name:"Heading 4 Char";
	mso-style-link:"Heading 4";
	font-family:"Times New Roman",serif;
	color:#E97132;
	font-style:italic;}
span.Heading5Char
	{mso-style-name:"Heading 5 Char";
	mso-style-link:"Heading 5";
	font-family:"Times New Roman",serif;
	color:#0F4761;}
span.Heading6Char
	{mso-style-name:"Heading 6 Char";
	mso-style-link:"Heading 6";
	font-family:"Times New Roman",serif;
	color:#595959;
	font-style:italic;}
span.Heading7Char
	{mso-style-name:"Heading 7 Char";
	mso-style-link:"Heading 7";
	font-family:"Times New Roman",serif;
	color:#595959;}
span.Heading8Char
	{mso-style-name:"Heading 8 Char";
	mso-style-link:"Heading 8";
	font-family:"Times New Roman",serif;
	color:#272727;
	font-style:italic;}
span.Heading9Char
	{mso-style-name:"Heading 9 Char";
	mso-style-link:"Heading 9";
	font-family:"Times New Roman",serif;
	color:#272727;}
span.TitleChar
	{mso-style-name:"Title Char";
	mso-style-link:Title;
	font-family:"Aptos Display",sans-serif;
	letter-spacing:-.5pt;}
span.SubtitleChar
	{mso-style-name:"Subtitle Char";
	mso-style-link:Subtitle;
	font-family:"Times New Roman",serif;
	color:#595959;
	letter-spacing:.75pt;}
span.QuoteChar
	{mso-style-name:"Quote Char";
	mso-style-link:Quote;
	color:#404040;
	font-style:italic;}
span.IntenseQuoteChar
	{mso-style-name:"Intense Quote Char";
	mso-style-link:"Intense Quote";
	color:#0F4761;
	font-style:italic;}
.MsoChpDefault
	{font-family:"Aptos",sans-serif;}
.MsoPapDefault
	{margin-bottom:8.0pt;
	line-height:115%;}
@page WordSection1
	{size:8.5in 11.0in;
	margin:1.0in 1.0in 1.0in 1.0in;}
div.WordSection1
	{page:WordSection1;}

 /* List Definitions */
 ol
	{margin-bottom:0in;}
ul
	{margin-bottom:0in;}

    /* local css */
    .custom-header {
            background-color:  #fb8500;
        }

         body {
            font-family: Arial, sans-serif;
            line-height: 1.5;
            margin: 20px;
        }
      

</style>

</head>

<body lang=EN-US>

    <header class="custom-header text-white text-center py-4"> <!-- Updated class here -->
        <h1>Paper Reviews</h1>
    </header>

<div class=WordSection1>

<h1 align=center style='text-align:center'><a name="_Toc193726717"><span
style='font-weight:normal'>ADVANCEMENTS IN VISION–LANGUAGE MODELS FOR REMOTE
SENSING: DATASETS, CAPABILITIES, AND ENHANCEMENT TECHNIQUES</span></a></h1>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b><span
style='font-size:18.0pt;font-family:"Arial",sans-serif;color:black'>&nbsp;</span></b></p>

<h2><a name="_Toc193726718">INDEX</a></h2>

<p class=MsoTocHeading>Table of Contents</p>

<p class=MsoToc1><span
class=MsoHyperlink><a href="#_Toc193726717">Advancements in Vision–Language
Models for Remote Sensing: Datasets, Capabilities, and Enhancement Techniques</a></span></p>

<p class=MsoToc2><span class=MsoHyperlink><a href="#_Toc193726718">INDEX</a></span></p>

<p class=MsoToc1><span class=MsoHyperlink><a href="#_Toc193726719">Resume</a></span></p>

<p class=MsoToc3><span class=MsoHyperlink><a href="#_Toc193726720">Conclusion</a></span></p>

<p class=MsoToc1><span class=MsoHyperlink><a href="#_Toc193726721"><span
style='background:yellow'>Note in detail:</span></a></span></p>

<p class=MsoToc2><span class=MsoHyperlink><a href="#_Toc193726722">INTRODUCTION</a></span></p>

<p class=MsoToc3><span class=MsoHyperlink><a href="#_Toc193726723">1.
Discriminative Models (Traditional AI Approaches)</a></span></p>

<p class=MsoToc3><span class=MsoHyperlink><a href="#_Toc193726724">2.
Vision–Language Models (VLMs: Generative AI)</a></span></p>

<p class=MsoToc2><span class=MsoHyperlink><a href="#_Toc193726725">Foundation
Models</a></span></p>

<p class=MsoToc3><span class=MsoHyperlink><a href="#_Toc193726726">1.
Transformer</a></span></p>

<p class=MsoToc3><span class=MsoHyperlink><a href="#_Toc193726727">2. Vision
Transformer (ViT)</a></span></p>

<p class=MsoToc3><span class=MsoHyperlink><a href="#_Toc193726728">3.
Vision–Language Models (VLMs)</a></span></p>

<p class=MsoToc2><span class=MsoHyperlink><a href="#_Toc193726729">DATASET</a></span></p>

<p class=MsoToc3><span class=MsoHyperlink><a href="#_Toc193726730">1. Manual
Datasets</a></span></p>

<p class=MsoToc3><span class=MsoHyperlink><a href="#_Toc193726731">2. Combined
Datasets</a></span></p>

<p class=MsoToc3><span class=MsoHyperlink><a href="#_Toc193726732">3.
Automatically Annotated Datasets</a></span></p>

<p class=MsoToc2><span class=MsoHyperlink><a href="#_Toc193726733">Capabilities</a></span></p>

<p class=MsoToc3><span class=MsoHyperlink><a href="#_Toc193726734">1. Pure
Visual Tasks</a></span></p>

<p class=MsoToc3><span class=MsoHyperlink><a href="#_Toc193726735">2.
Vision–Language Tasks</a></span></p>

<p class=MsoToc2><span class=MsoHyperlink><a href="#_Toc193726736">RECENT
ADVANCES</a></span></p>

<p class=MsoToc3><span class=MsoHyperlink><a href="#_Toc193726737">1.
Contrastive VLMs</a></span></p>

<p class=MsoToc3><span class=MsoHyperlink><a href="#_Toc193726738">2. Advanced
Conversational VLMs</a></span></p>

<p class=MsoToc3><span class=MsoHyperlink><a href="#_Toc193726739">3.
Specialized Models</a></span></p>

<p class=MsoToc2><span class=MsoHyperlink><a href="#_Toc193726740">GAPS AND
FUTURE WORK IN VISION–LANGUAGE MODELS (VLMS) FOR REMOTE SENSING</a></span></p>

<p class=MsoToc3><span class=MsoHyperlink><a href="#_Toc193726741">Identified
Gaps (Current Shortcomings)</a></span></p>

<p class=MsoToc3><span class=MsoHyperlink><a href="#_Toc193726742">Future Work
Directions</a></span></p>

<p class=MsoToc3><span class=MsoHyperlink><a href="#_Toc193726743">Additional
Considerations</a></span></p>

<p class=MsoToc3><span class=MsoHyperlink><a href="#_Toc193726744">Conclusion</a></span></p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'>&nbsp;</p>

<h1 align=center style='text-align:center'><a name="_Toc193726719"><span
style='font-weight:normal'>Resume</span></a></h1>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>1. </b><span
class=Heading3Char><span style='font-size:14.0pt;font-family:"Aptos",sans-serif'>General
Idea</span></span></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Main
     Focus/Research Question</b>: The paper reviews advancements in
     Vision–Language Models (VLMs) for remote sensing, focusing on their
     capabilities, datasets, and enhancement techniques. It evaluates how VLMs
     address limitations of traditional discriminative models in tasks like
     geophysical classification, object detection, and scene understanding. </li>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Significance</b>:
     VLMs bridge visual and linguistic data, enabling multi-task learning,
     human-like reasoning, and zero-/few-shot adaptability. This represents a
     transition from task-specific AI to flexible generative AI in remote
     sensing, with applications in disaster monitoring, urban planning, and
     environmental management. </li>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Contribution</b>:
     Provides a structured review of VLM architectures, datasets, and
     enhancement strategies specific to remote sensing, highlighting their
     potential to generalize across tasks and integrate language-guided
     reasoning.</li>
</ul>

<p class=MsoNormal style='margin-top:0in;margin-right:0in;margin-bottom:0in;
margin-left:.5in;line-height:normal'>&nbsp;</p>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>2. </b><span
class=Heading3Char><span style='font-size:14.0pt;font-family:"Aptos",sans-serif'>Methodology</span></span></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Methods</b>:
     A systematic literature review using Google Scholar with keywords like <i>“Visual
     language models for remote sensing.”</i> </li>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Experimental
     Design:</b></li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Dataset
      Categorization</b>: Classifies datasets into <i>manual</i> (high-quality,
      task-specific), <i>combined</i> (merged existing datasets), and <i>automatically
      annotated</i> (generated via VLMs/LLMs like GPT-4). </li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Model
      Analysis</b>: Compares contrastive (e.g., CLIP-based) and conversational
      (e.g., LLaVA-based) VLM frameworks. </li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Performance
      Evaluation</b>: Benchmarks models on tasks (e.g., VQA, image captioning)
      using datasets like AID, NWPU-RESISC45, and LEVIR-CD.</li>
 </ul>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Tools/Algorithms</b>:
     CLIP, ViT, LLaVA, Vicuna, GPT-4, and specialized models like RemoteCLIP
     and SkySenseGPT.</li>
</ul>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'>&nbsp;</p>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>3. </b><span
class=Heading3Char><span style='font-size:14.0pt;font-family:"Aptos",sans-serif'>Gaps
from Previous Work</span></span></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Limitations
     of Prior Methods</b>: Traditional discriminative models (e.g., CNNs,
     U-Net) were single-task, lacked multimodal integration, and struggled with
     long-tail distributions or commonsense reasoning. </li>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Proposed
     Method Differences</b>: VLMs unify tasks into generative frameworks,
     aligning language and vision for multi-task flexibility and human-like
     interaction. They incorporate pre-trained LLMs (e.g., Vicuna) and
     domain-specific adapters (e.g., Q-Formers) for improved generalization. </li>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Shortcomings
     Addressed</b>: Overcomes rigid task boundaries, poor zero-shot
     adaptability, and limited scalability of earlier AI models.</li>
</ul>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'>&nbsp;</p>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>4. </b><span
class=Heading3Char><span style='font-size:14.0pt;font-family:"Aptos",sans-serif'>Rationale
for Proposed Method</span></span></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Method
     Choice</b>: VLMs were selected for their ability to merge language and
     vision, enabling open-ended reasoning and multi-task learning. Pre-trained
     LLMs (e.g., LLaMA) reduce training costs, while alignment layers (MLPs,
     attention mechanisms) bridge modalities. </li>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Theoretical/Practical
     Justification</b>: Aligns with the need for models that handle diverse
     remote sensing data (e.g., SAR, hyperspectral) and complex queries.
     Automatically annotated datasets (e.g., RS5M) scale training while
     minimizing manual effort. </li>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Alignment
     with Gaps</b>: Addresses prior limitations by leveraging generative
     frameworks, cross-modal alignment, and large-scale pretraining.</li>
</ul>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'>&nbsp;</p>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>5. </b><span
class=Heading3Char><span style='font-size:14.0pt;font-family:"Aptos",sans-serif'>Advantages
of the Proposed Method</span></span></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Key Benefits:</b></li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Multi-task
      Flexibility</b>: Handles classification, captioning, VQA, and grounding
      in a single framework. </li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Generalization</b>:
      Pre-training on diverse datasets (e.g., RS5M) improves zero-shot
      performance. </li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Performance
      Metrics</b>: Conversational VLMs (e.g., SkySenseGPT) achieve 95.1%
      accuracy on AID for scene classification and outperform contrastive
      models in VQA (e.g., 79.6% on RSVQA-HR).</li>
 </ul>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Impact</b>:
     Enables region-specific dialogue (GeoChat), time-series analysis
     (Changen2), and high-precision captioning (RSICap).</li>
</ul>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'>&nbsp;</p>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>6. </b><span
class=Heading3Char><span style='font-size:14.0pt;font-family:"Aptos",sans-serif'>Future
Work</span></span></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Author Suggestions:</b></li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Regression
      Tasks</b>: Improve numerical tokenization and integrate regression heads
      (e.g., REO-VLM’s MLP-mixer). </li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Multispectral/SAR
      Adaptation</b>: Develop domain-specific feature extractors and pseudo-RGB
      strategies for non-RGB data. </li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Multimodal
      Outputs</b>: Enable image/video generation (e.g., Emu3’s next-token
      prediction) and agent-based activation of specialized models. </li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Temporal
      Analysis</b>: Incorporate multitemporal data for trend inference (e.g.,
      climate change monitoring).</li>
 </ul>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Unresolved
     Challenges</b>: Domain-specific benchmarks, efficient training for large
     models, and handling rare/ambiguous annotations.</li>
</ul>

<h3><a name="_Toc193726720">Conclusion</a></h3>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Contributions</b>:
     This review systematizes VLM advancements in remote sensing, highlighting
     their transition from discriminative to generative AI. It catalogs
     datasets (e.g., RS5M, RSICap), model architectures (e.g., contrastive vs.
     conversational), and enhancement techniques (e.g., alignment layers). </li>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Impact</b>:
     VLMs enable scalable, flexible solutions for geospatial analysis, with
     implications for disaster response, urban planning, and environmental
     monitoring. Future progress hinges on addressing domain-specific data
     challenges and expanding into temporal/multimodal reasoning. </li>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Implications</b>:
     Establishes VLMs as foundational tools for next-gen remote sensing AI,
     bridging vision, language, and domain expertise.</li>
</ul>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'>&nbsp;</p>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'>&nbsp;</p>

<h1><a name="_Toc193726721"><span style='background:yellow'>Note in detail:</span></a></h1>

<h2><a name="_Toc193726722">INTRODUCTION</a></h2>

<h3><a name="_Toc193726723">1. Discriminative Models (Traditional AI
Approaches)</a></h3>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>(A)
Convolutional Neural Networks (CNNs)</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>3D CNNs</b></li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Purpose</b>:
      Designed for hyperspectral and multispectral image classification and
      spectral reconstruction. </li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Pros:</b></li>
  <ul style='margin-top:0in' type=square>
   <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Captures
       spatial-spectral features simultaneously. </li>
   <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Effective
       for volumetric data (e.g., hyperspectral cubes).</li>
  </ul>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Cons:</b></li>
  <ul style='margin-top:0in' type=square>
   <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Computationally
       expensive due to 3D convolutions. </li>
   <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Limited
       scalability to very high-resolution imagery.</li>
  </ul>
 </ul>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>U-Net</b></li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Purpose</b>:
      Semantic segmentation and land cover mapping in aerial imagery. </li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Pros:</b></li>
  <ul style='margin-top:0in' type=square>
   <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Skip
       connections preserve spatial details. </li>
   <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Works well
       with limited labeled data.</li>
  </ul>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Cons:</b></li>
  <ul style='margin-top:0in' type=square>
   <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Struggles
       with fine-grained object boundaries in very high-resolution images. </li>
   <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Requires
       extensive training for domain adaptation.</li>
  </ul>
 </ul>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Pyramid
     Networks for SAR Images</b></li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Purpose</b>:
      Multi-scale object detection in Synthetic Aperture Radar (SAR) imagery. </li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Pros:</b></li>
  <ul style='margin-top:0in' type=square>
   <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Handles
       scale variation in SAR targets (e.g., ships, buildings). </li>
   <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Robust to
       speckle noise.</li>
  </ul>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Cons:</b></li>
  <ul style='margin-top:0in' type=square>
   <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Complex
       architecture with high memory usage. </li>
   <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Limited
       adaptability to rare object classes.</li>
  </ul>
 </ul>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>YOLO
     Framework</b></li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Purpose</b>:
      Small target detection in infrared remote sensing. </li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Pros:</b></li>
  <ul style='margin-top:0in' type=square>
   <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Real-time
       inference. </li>
   <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Efficient
       for detecting small objects (e.g., vehicles, aircraft).</li>
  </ul>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Cons:</b></li>
  <ul style='margin-top:0in' type=square>
   <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Struggles
       with densely clustered targets. </li>
   <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Lower
       precision in low-contrast scenarios (e.g., foggy or cloudy conditions).</li>
  </ul>
 </ul>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>FFCA-YOLO</b></li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Purpose</b>:
      Enhances small object detection with plug-and-play modules for feature
      fusion and context awareness. </li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Pros:</b></li>
  <ul style='margin-top:0in' type=square>
   <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Boosts
       local/global feature correlation without increasing complexity. </li>
   <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Maintains
       computational efficiency.</li>
  </ul>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Cons:</b></li>
  <ul style='margin-top:0in' type=square>
   <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Still
       inherits YOLO’s limitations in complex backgrounds.</li>
  </ul>
 </ul>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Improved
     YOLOv5 (Zhang et al.)</b></li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Purpose</b>:
      Enhanced object detection via spatial-to-depth (SPD) and CoTC3 modules. </li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Pros:</b></li>
  <ul style='margin-top:0in' type=square>
   <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Improves
       contextual information utilization. </li>
   <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Better
       performance on multi-scale targets.</li>
  </ul>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Cons:</b></li>
  <ul style='margin-top:0in' type=square>
   <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Increased
       parameter count may reduce efficiency.</li>
  </ul>
 </ul>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>MLPA
     (Multi-Level Feature Alignment)</b></li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Purpose</b>:
      Cross-domain hyperspectral image (HSI) classification. </li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Pros:</b></li>
  <ul style='margin-top:0in' type=square>
   <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Aligns
       features across domains (e.g., satellite to UAV). </li>
   <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Improves
       generalization to unseen environments.</li>
  </ul>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Cons:</b></li>
  <ul style='margin-top:0in' type=square>
   <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Requires
       paired data for alignment. </li>
   <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Computational
       overhead during training.</li>
  </ul>
 </ul>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>LBA-MCNet</b></li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Purpose</b>:
      Object Saliency Detection (ORSI-SOD) with boundary refinement. </li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Pros:</b></li>
  <ul style='margin-top:0in' type=square>
   <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Balances
       foreground/background context modeling. </li>
   <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Effective
       for ambiguous object edges.</li>
  </ul>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Cons:</b></li>
  <ul style='margin-top:0in' type=square>
   <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Sensitive
       to initial segmentation quality.</li>
  </ul>
 </ul>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>MSC-GAN</b></li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Purpose</b>:
      Multitemporal cloud removal via generative adversarial networks (GANs). </li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Pros:</b></li>
  <ul style='margin-top:0in' type=square>
   <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Reconstructs
       cloud-free images using temporal data. </li>
   <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Efficient
       feature interaction reduces artifacts.</li>
  </ul>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Cons</b>:
      </li>
  <ul style='margin-top:0in' type=square>
   <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Struggles
       with thick cloud coverage. </li>
   <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Requires
       aligned multitemporal inputs.</li>
  </ul>
 </ul>
</ol>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'>&nbsp;</p>

<h3><a name="_Toc193726724">2. Vision–Language Models (VLMs: Generative AI)</a></h3>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>CLIP</b></li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Purpose</b>:
      Aligns images and text for cross-modal tasks (e.g., zero-shot
      classification). </li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Pros</b>:
      </li>
  <ul style='margin-top:0in' type=square>
   <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Generalizes
       across datasets without fine-tuning. </li>
   <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Multimodal
       understanding improves interpretability.</li>
  </ul>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Cons</b>:
      </li>
  <ul style='margin-top:0in' type=square>
   <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Limited
       spatial resolution awareness. </li>
   <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Text
       prompts must be carefully designed for remote sensing.</li>
  </ul>
 </ul>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>LLaVA/GPT-4</b></li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Purpose</b>:
      Combines visual encoders (e.g., ViT) with large language models (LLMs)
      for open-ended vision-language tasks. </li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Pros</b>:
      </li>
  <ul style='margin-top:0in' type=square>
   <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Supports
       multi-task workflows (e.g., VQA, captioning). </li>
   <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Human-like
       reasoning for geospatial analysis.</li>
  </ul>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Cons:</b></li>
  <ul style='margin-top:0in' type=square>
   <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Computationally
       intensive (requires GPUs). </li>
   <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Fine-tuning
       needed for domain-specific terms (e.g., SAR).</li>
  </ul>
 </ul>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>GeoChat</b></li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Purpose</b>:
      Regional dialogue and geographic queries (e.g., administrative
      boundaries). </li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Pros:</b></li>
  <ul style='margin-top:0in' type=square>
   <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Accepts
       region-based inputs for localized analysis. </li>
   <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Integrates
       geospatial knowledge (e.g., OpenStreetMap).</li>
  </ul>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Cons</b>:
      </li>
  <ul style='margin-top:0in' type=square>
   <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Early-stage
       tool with limited real-world adoption.</li>
  </ul>
 </ul>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>RSICap</b></li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Purpose</b>:
      Remote sensing image captioning via high-quality annotated datasets. </li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Pros:</b></li>
  <ul style='margin-top:0in' type=square>
   <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Enables
       human-readable descriptions for RS imagery. </li>
   <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Supports
       model fine-tuning for domain adaptation.</li>
  </ul>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Cons:</b></li>
  <ul style='margin-top:0in' type=square>
   <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Manual
       annotation is labor-intensive.</li>
  </ul>
 </ul>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Changen2</b></li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Purpose</b>:
      Generates time-series imagery with change labels for multitemporal
      analysis. </li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Pros</b>:
      </li>
  <ul style='margin-top:0in' type=square>
   <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Reduces
       annotation costs via synthetic data. </li>
   <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Useful for
       disaster monitoring (e.g., deforestation).</li>
  </ul>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Cons</b>:
      </li>
  <ul style='margin-top:0in' type=square>
   <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Synthetic
       data may lack real-world complexity.</li>
  </ul>
 </ul>
</ol>

<p class=MsoNormal align=center style='margin-bottom:0in;text-align:center;
line-height:normal'><b>&nbsp;</b></p>

<p class=MsoNormal align=center style='margin-bottom:0in;text-align:center;
line-height:normal'><b>&nbsp;</b></p>

<p class=MsoNormal align=center style='margin-bottom:0in;text-align:center;
line-height:normal'><b>Comparison: Discriminative vs. Generative Models</b></p>

<table class=MsoNormalTable border=1 cellspacing=0 cellpadding=0 width=637
 style='width:477.65pt;border-collapse:collapse;border:none'>
 <thead>
  <tr style='height:43.7pt'>
   <td valign=bottom style='border:solid windowtext 1.0pt;padding:0in 0in 6.85pt 0in;
   height:43.7pt'>
   <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>ASPECT</b></p>
   </td>
   <td valign=bottom style='border:solid windowtext 1.0pt;border-left:none;
   padding:0in 0in 6.85pt 0in;height:43.7pt'>
   <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>DISCRIMINATIVE
   MODELS</b></p>
   </td>
   <td valign=bottom style='border:solid windowtext 1.0pt;border-left:none;
   padding:0in 0in 6.85pt 0in;height:43.7pt'>
   <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>VISION–LANGUAGE
   MODELS (VLMS)</b></p>
   </td>
  </tr>
 </thead>
 <tr style='height:41.95pt'>
  <td valign=bottom style='border:solid windowtext 1.0pt;border-top:none;
  padding:6.85pt 0in 6.85pt 0in;height:41.95pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Task
  Flexibility</b></p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  41.95pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Single-task
  (e.g., classification, detection).</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  41.95pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Multi-task
  (e.g., VQA, captioning, retrieval).</p>
  </td>
 </tr>
 <tr style='height:43.7pt'>
  <td valign=bottom style='border:solid windowtext 1.0pt;border-top:none;
  padding:6.85pt 0in 6.85pt 0in;height:43.7pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Data
  Requirements</b></p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  43.7pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Label-intensive
  for specific tasks.</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  43.7pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Leverages
  unlabeled data via pre-training.</p>
  </td>
 </tr>
 <tr style='height:41.95pt'>
  <td valign=bottom style='border:solid windowtext 1.0pt;border-top:none;
  padding:6.85pt 0in 6.85pt 0in;height:41.95pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Adaptability</b></p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  41.95pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Limited to trained
  domains.</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  41.95pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Zero/few-shot
  learning; better generalization.</p>
  </td>
 </tr>
 <tr style='height:43.7pt'>
  <td valign=bottom style='border:solid windowtext 1.0pt;border-top:none;
  padding:6.85pt 0in 6.85pt 0in;height:43.7pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Interpretability</b></p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  43.7pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Low
  (black-box predictions).</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  43.7pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>High
  (human-like explanations via text).</p>
  </td>
 </tr>
 <tr style='height:43.7pt'>
  <td valign=bottom style='border:solid windowtext 1.0pt;border-top:none;
  padding:6.85pt 0in 6.85pt 0in;height:43.7pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Computation</b></p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  43.7pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Efficient for
  edge deployment.</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  43.7pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Requires
  heavy computational resources.</p>
  </td>
 </tr>
</table>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'>&nbsp;</p>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Conclusion</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Discriminative
     Models</b> excel in fast, task-specific applications (e.g., object
     detection) but lack flexibility and require extensive labeled data. </li>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Vision–Language
     Models</b> address multi-task challenges and enable human-AI collaboration
     but face computational and domain-adaptation hurdles. Future work in
     remote sensing VLMs should focus on efficiency (e.g., lightweight LLMs),
     domain-specific pretraining (e.g., SAR/HSI alignment), and multimodal
     output (e.g., maps, 3D models).</li>
</ul>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'>&nbsp;</p>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><img width=24
height=24 src="Advancements%20in%20Vision–Language%20Models.fld/image001.png"
alt="Remotesensing 17 00162 g001"><img width=24 height=24
src="Advancements%20in%20Vision–Language%20Models.fld/image001.png"
alt="Remotesensing 17 00162 g001"><img border=0 width=374 height=217
src="Advancements%20in%20Vision–Language%20Models.fld/image002.png"></p>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><img border=0
width=385 height=245
src="Advancements%20in%20Vision–Language%20Models.fld/image003.png"
alt="A diagram of a survey&#10;&#10;AI-generated content may be incorrect."></p>

<p class=MsoNormal align=center style='margin-bottom:0in;text-align:center;
line-height:normal'>&nbsp;</p>

<h2><a name="_Toc193726725">Foundation Models</a></h2>

<h3><a name="_Toc193726726">1. Transformer</a></h3>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Overview:<br>
</b>The Transformer is a neural network architecture introduced in 2017 for
natural language processing (NLP). It replaces traditional sequential models
(e.g., RNNs, LSTMs) with a parallelizable self-attention mechanism. </p>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><img border=0
width=336 height=432
src="Advancements%20in%20Vision–Language%20Models.fld/image004.png"
alt="A diagram of a process&#10;&#10;AI-generated content may be incorrect."></p>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Key Components:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Encoder-Decoder
     Structure:</b></li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Encoder:
      Processes input tokens via self-attention and feed-forward layers. </li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Decoder:
      Generates output tokens using masked self-attention (to prevent future
      token leakage) and cross-attention (to focus on encoder outputs).</li>
 </ul>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Attention Mechanism:</b></li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Self-Attention:
      Computes relationships between all input tokens simultaneously using
      Query (Q), Key (K), and Value (V) matrices. </li>
  <ul style='margin-top:0in' type=square>
   <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Formula:<br>
       Attention(<i>Q</i>,<i>K</i>,<i>V</i>)=Softmax(<i>dk</i><span
       style='font-family:"Arial",sans-serif'>​​</span><i>QKT</i><span
       style='font-family:"Arial",sans-serif'>​</span>)<i>V</i></li>
   <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Scaling
       Factor (<i>dk</i><span style='font-family:"Arial",sans-serif'>​​</span>):
       Normalizes dot products to stabilize gradients.</li>
  </ul>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Cross-Attention:
      Uses Q from one sequence (e.g., decoder) and K, V from another (e.g.,
      encoder).</li>
 </ul>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Feed-Forward
     Network (FFN): </b></li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Two linear
      layers with ReLU activation, expanding dimensions (e.g., 4× input width),
      and contracting back.</li>
 </ul>
</ul>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Advantages:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Parallel
     Processing: Unlike RNNs, processes all tokens simultaneously, speeding up
     training. </li>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Long-Range
     Dependencies: Captures relationships between distant tokens (e.g.,
     sentence context). </li>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Scalability:
     Foundation for large language models (LLMs) like GPT and BERT.</li>
</ul>

<h3><a name="_Toc193726727">2. Vision Transformer (ViT)</a></h3>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Adaptation for
Images:<br>
ViT applies Transformer architecture to image processing by treating images as
sequences of patches. </p>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><img border=0
width=468 height=234
src="Advancements%20in%20Vision–Language%20Models.fld/image005.png"
alt="A diagram of a transformer&#10;&#10;AI-generated content may be incorrect."></p>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Key Steps:</b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Patch Embedding:</li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Splits an
      image into fixed-size patches (e.g., 16×16 pixels). </li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Flattens
      patches into vectors and projects them into embeddings using a linear
      layer.</li>
 </ul>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Position Embeddings:</li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Adds
      learnable positional encodings to retain spatial information. </li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Types:
      Absolute (fixed positions), Relative (distance-based), Rotary
      (rotation-sensitive).</li>
 </ul>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Transformer Encoder:</li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Processes
      patch embeddings through self-attention and FFN layers.</li>
 </ul>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Classification
     Head (MLP): </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Aggregates
      global features for tasks like image classification.</li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>&nbsp;</li>
 </ul>
</ol>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Advantages
Over CNNs:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Scalability:
     Performance improves with larger models/data (no saturation). </li>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Global
     Context: Self-attention captures relationships across the entire image.</li>
</ul>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Variants: </b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>SwinTransformer:
     Hierarchical attention with shifted windows for efficiency. </li>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>DeiT:
     Distillation-based training for data-efficient ViTs.</li>
</ul>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>&nbsp;</b></p>

<h3><a name="_Toc193726728">3. Vision–Language Models (VLMs)</a></h3>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Unifying Vision
and Language:<br>
VLMs integrate visual (ViT) and textual (LLM) processing for multimodal tasks
(e.g., captioning, VQA). </p>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'>&nbsp;</p>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><img border=0
width=468 height=198
src="Advancements%20in%20Vision–Language%20Models.fld/image006.png"
alt="A diagram of a process&#10;&#10;AI-generated content may be incorrect."></p>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'>&nbsp;</p>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><img border=0
width=468 height=363
src="Advancements%20in%20Vision–Language%20Models.fld/image007.png"
alt="A screenshot of a computer&#10;&#10;AI-generated content may be incorrect."></p>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Example
Architecture (LLaVA): </b></p>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Visual
     Encoder: </b></li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Uses
      CLIP-ViT to extract image features, pre-trained on image-text pairs for
      alignment.</li>
 </ul>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Projection
     Layer</b>: </li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Maps visual
      features to language-embedding space (e.g., via linear layers).</li>
 </ul>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Large
     Language Model (LLM): </b></li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Processes
      combined visual and text tokens (e.g., Vicuna, LLaMA). </li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Generates
      text outputs (e.g., answers, descriptions) based on multimodal input.</li>
 </ul>
</ol>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Key Strengths:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Multitasking:
     </b>Handles diverse tasks (classification, captioning, reasoning) in one
     framework. </li>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Zero/Few-Shot
     Learning:</b> Leverages pre-trained knowledge for unseen tasks (e.g.,
     GPT-4). </li>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Human
     Interaction:</b> Enables conversational interfaces (e.g., region-specific
     queries in GeoChat).</li>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>&nbsp;</li>
</ul>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Example Applications:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>GeoChat:
     Combines ViT with LLM for geospatial analysis (e.g., boundary mapping). </li>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>CLIP: Aligns
     image-text pairs for zero-shot classification.</li>
</ul>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>&nbsp;</b></p>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Summary</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Transformer</b>:
     Revolutionized NLP with parallelizable self-attention. </li>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>ViT</b>:
     Adapted Transformers for vision via patch embeddings, outperforming CNNs
     at scale. </li>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>VLMs</b>:
     Bridge vision-language modalities (e.g., LLaVA) for flexible, human-like
     AI tasks.<br>
     <b>Impact</b>: Foundation for generative AI (e.g., ChatGPT, DALL-E) and
     domain-specific tools (e.g., remote sensing VLMs).</li>
</ul>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>&nbsp;</b></p>

<h2><a name="_Toc193726729">DATASET</a></h2>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b><img
border=0 width=468 height=376
src="Advancements%20in%20Vision–Language%20Models.fld/image008.png"
alt="A diagram of data processing&#10;&#10;AI-generated content may be incorrect."></b></p>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>&nbsp;</b></p>

<h3><a name="_Toc193726730">1. Manual Datasets</a></h3>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Description:
     </b>Expert-curated, small-scale datasets with high-quality annotations. </li>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Examples:</b></li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>HallusionBench:
      Tests VLM robustness via 455 visual-QA pairs (346 images, 1129
      questions). </li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>RSICap’s
      RSIEval: Provides five expert captions per image for fine-tuning. </li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>CRSVQA: Uses
      domain experts to craft complex questions, reducing language bias.</li>
 </ul>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Pros:</b></li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>High
      accuracy and task alignment. </li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Minimizes
      redundancy and bias.</li>
 </ul>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Cons:</b></li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Time-consuming,
      expensive, and small scale. </li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Limited
      generalization due to size.</li>
 </ul>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Use Case: </b>Fine-tuning
     VLM models for specific tasks.</li>
</ul>

<p class=MsoNormal style='margin-top:0in;margin-right:0in;margin-bottom:0in;
margin-left:.5in;line-height:normal'><b>&nbsp;</b></p>

<h3><a name="_Toc193726731">2. Combined Datasets</a></h3>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Description:
     </b>Merges existing RS datasets (e.g., Sentinel-2, AID, DIOR) to create
     large-scale resources. </li>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Examples:</b></li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>AID/NWPU-RESISC45:
      Scene classification. </li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>FAIR1M/DIOR:
      Object detection. </li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>LEVIR-CD:
      Change detection. </li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Million-AID:
      Benchmark dataset.</li>
 </ul>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Pros:</b></li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Cost-effective
      and scalable (millions of entries). </li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Facilitates
      multi-task learning.</li>
 </ul>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Cons:</b></li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Requires
      preprocessing (format/resolution alignment). </li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Lower
      annotation quality compared to manual datasets.</li>
 </ul>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Use Case:
     Pre-training VLMs on diverse RS tasks.</b></li>
</ul>

<p class=MsoNormal style='margin-top:0in;margin-right:0in;margin-bottom:0in;
margin-left:.5in;line-height:normal'><b>&nbsp;</b></p>

<h3><a name="_Toc193726732">3. Automatically Annotated Datasets</a></h3>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Description:
     </b>Leverages LLMs/VLMs (e.g., BLIP2, GPT-4, CLIP) to generate image-text
     pairs at scale. </li>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Examples:</b></li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>RS5M:
      Auto-generates captions using BLIP2 and CLIP filtering. </li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>SkyScript:
      Matches OSM attributes with Google Images. </li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>GeoChat/GPT-4:
      Creates dialogue datasets via prompts. </li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>HqDC-1.4M:
      Uses Gemini for multi-dataset captioning.</li>
 </ul>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Pros:</b></li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Massive
      scale with flexible annotations (captions, Q&amp;A pairs). </li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Cost-efficient
      and adaptable to new tasks.</li>
 </ul>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Cons:</b></li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Risk of
      model-induced errors/bias. </li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Requires
      hybrid human-AI validation.</li>
 </ul>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Use Case:
     Mainstream pre-training and task-specific adaptation (e.g., disaster
     monitoring).</b></li>
</ul>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>&nbsp;</b></p>

<h3>Comparison</h3>

<table class=MsoNormalTable border=1 cellspacing=0 cellpadding=0 width=577
 style='width:432.45pt;border-collapse:collapse;border:none'>
 <thead>
  <tr style='height:12.25pt'>
   <td valign=bottom style='border:solid windowtext 1.0pt;padding:0in 0in 6.85pt 0in;
   height:12.25pt'>
   <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>TYPE</b></p>
   </td>
   <td valign=bottom style='border:solid windowtext 1.0pt;border-left:none;
   padding:0in 0in 6.85pt 0in;height:12.25pt'>
   <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>SCALE</b></p>
   </td>
   <td valign=bottom style='border:solid windowtext 1.0pt;border-left:none;
   padding:0in 0in 6.85pt 0in;height:12.25pt'>
   <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>QUALITY</b></p>
   </td>
   <td valign=bottom style='border:solid windowtext 1.0pt;border-left:none;
   padding:0in 0in 6.85pt 0in;height:12.25pt'>
   <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>COST</b></p>
   </td>
   <td valign=bottom style='border:solid windowtext 1.0pt;border-left:none;
   padding:0in 0in 6.85pt 0in;height:12.25pt'>
   <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>BEST FOR</b></p>
   </td>
  </tr>
 </thead>
 <tr style='height:13.15pt'>
  <td valign=bottom style='border:solid windowtext 1.0pt;border-top:none;
  padding:6.85pt 0in 6.85pt 0in;height:13.15pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Manual</b></p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  13.15pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Small</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  13.15pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>High (expert)</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  13.15pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>High</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  13.15pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Task-specific
  fine-tuning.</p>
  </td>
 </tr>
 <tr style='height:12.25pt'>
  <td valign=bottom style='border:solid windowtext 1.0pt;border-top:none;
  padding:6.85pt 0in 6.85pt 0in;height:12.25pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Combined</b></p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  12.25pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Large</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  12.25pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Moderate</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  12.25pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Low</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  12.25pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Pre-training,
  multi-task.</p>
  </td>
 </tr>
 <tr style='height:13.15pt'>
  <td valign=bottom style='border:solid windowtext 1.0pt;border-top:none;
  padding:6.85pt 0in 6.85pt 0in;height:13.15pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Auto-Annotated</b></p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  13.15pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Massive</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  13.15pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Moderate-High</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  13.15pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Low</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  13.15pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Scalable
  pre-training, diverse tasks.</p>
  </td>
 </tr>
</table>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>&nbsp;</b></p>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Key
Takeaways</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Manual
     Datasets: Critical for niche tasks requiring precision but impractical for
     large models. </li>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Combined
     Datasets: Balance scale and diversity but need heavy preprocessing. </li>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Auto-Annotated
     Datasets: Dominant due to scalability and LLM efficiency, though quality
     control via hybrid validation is essential. </li>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Trend:
     Auto-annotated datasets are becoming the cornerstone of VLM development in
     RS, supplemented by manual data for refinement.</li>
</ul>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>&nbsp;</b></p>

<h2><a name="_Toc193726733">Capabilities</a></h2>

<p class=MsoListParagraph style='margin-top:0in;margin-right:0in;margin-bottom:
0in;margin-left:.25in;line-height:normal'><b>&nbsp;</b></p>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b><img
border=0 width=468 height=462
src="Advancements%20in%20Vision–Language%20Models.fld/image009.png"
alt="A screenshot of a computer program&#10;&#10;AI-generated content may be incorrect."></b></p>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>&nbsp;</b></p>

<h3><a name="_Toc193726734">1. Pure Visual Tasks</a></h3>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Tasks that
analyze remote sensing imagery <i>without textual input</i> to extract
geospatial insights. </p>

<table class=MsoNormalTable border=1 cellspacing=0 cellpadding=0 width=625
 style='width:468.6pt;border-collapse:collapse;border:none'>
 <thead>
  <tr style='height:14.15pt'>
   <td valign=bottom style='border:solid windowtext 1.0pt;padding:0in 0in 6.85pt 0in;
   height:14.15pt'>
   <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>TASK</b></p>
   </td>
   <td valign=bottom style='border:solid windowtext 1.0pt;border-left:none;
   padding:0in 0in 6.85pt 0in;height:14.15pt'>
   <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>PURPOSE</b></p>
   </td>
   <td valign=bottom style='border:solid windowtext 1.0pt;border-left:none;
   padding:0in 0in 6.85pt 0in;height:14.15pt'>
   <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>KEY
   DATASETS</b></p>
   </td>
   <td valign=bottom style='border:solid windowtext 1.0pt;border-left:none;
   padding:0in 0in 6.85pt 0in;height:14.15pt'>
   <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>DATASET
   FEATURES</b></p>
   </td>
   <td valign=bottom style='border:solid windowtext 1.0pt;border-left:none;
   padding:0in 0in 6.85pt 0in;height:14.15pt'>
   <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>APPLICATIONS</b></p>
   </td>
  </tr>
 </thead>
 <tr style='height:59.75pt'>
  <td valign=bottom style='border:solid windowtext 1.0pt;border-top:none;
  padding:6.85pt 0in 6.85pt 0in;height:59.75pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Scene
  Classification (SC)</b></p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  59.75pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Classify
  images into land-use/land-cover categories.</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  59.75pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>AID,
  NWPU-RESISC45, UCM</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  59.75pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>AID: 10k
  images, 30 classes; NWPU-RESISC45: 31.5k images, 45 classes; UCM: 2.1k
  images, 21 classes.</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  59.75pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Urban
  planning, environmental monitoring.</p>
  </td>
 </tr>
 <tr style='height:44.55pt'>
  <td valign=bottom style='border:solid windowtext 1.0pt;border-top:none;
  padding:6.85pt 0in 6.85pt 0in;height:44.55pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Object
  Detection (OD)</b></p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  44.55pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Detect and
  localize objects (e.g., buildings, vehicles).</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  44.55pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>DOTA, DIOR</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  44.55pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>DOTA: 2,806 aerial
  images, 15 classes; DIOR: 23,463 images, 20 classes, 192k+ instances.</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  44.55pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Traffic
  monitoring, military reconnaissance.</p>
  </td>
 </tr>
 <tr style='height:43.55pt'>
  <td valign=bottom style='border:solid windowtext 1.0pt;border-top:none;
  padding:6.85pt 0in 6.85pt 0in;height:43.55pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Semantic
  Segmentation (SS)</b></p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  43.55pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Assign
  pixel-level labels to objects in images.</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  43.55pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>ISPRS
  Vaihingen, iSAID</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  43.55pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>ISPRS:
  High-res (5 cm) TOP images; iSAID: 2,806 aerial images from Google Earth.</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  43.55pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Disaster
  assessment, crop yield estimation.</p>
  </td>
 </tr>
 <tr style='height:44.55pt'>
  <td valign=bottom style='border:solid windowtext 1.0pt;border-top:none;
  padding:6.85pt 0in 6.85pt 0in;height:44.55pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Change
  Detection (CD)</b></p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  44.55pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Identify
  changes (e.g., urban expansion, deforestation) in multitemporal images.</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  44.55pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>LEVIR-CD,
  AICD, Google Data Set</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  44.55pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>LEVIR-CD: Building-focused
  changes; AICD: Synthetic dataset for algorithm testing.</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  44.55pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Environmental
  monitoring, damage assessment.</p>
  </td>
 </tr>
 <tr style='height:44.55pt'>
  <td valign=bottom style='border:solid windowtext 1.0pt;border-top:none;
  padding:6.85pt 0in 6.85pt 0in;height:44.55pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Object
  Counting (OC)</b></p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  44.55pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Estimate the
  number of objects (e.g., vehicles, trees) in images.</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  44.55pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>RemoteCount
  (based on DOTA validation set)</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  44.55pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Derived from
  DOTA dataset; manually annotated for counting.</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  44.55pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Urban
  planning, wildlife conservation.</p>
  </td>
 </tr>
</table>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>&nbsp;</b></p>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Key
Strengths of VLMs:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Enable
     zero-shot/few-shot learning (e.g., classify unseen land-cover types). </li>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Handle rare
     objects via pretraining on large-scale datasets like DIOR.</li>
</ul>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>&nbsp;</b></p>

<h3><a name="_Toc193726735">2. Vision–Language Tasks</a></h3>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Tasks that <i>combine
imagery with natural language</i> for multimodal reasoning. </b></p>

<table class=MsoNormalTable border=1 cellspacing=0 cellpadding=0 width=654
 style='width:490.55pt;border-collapse:collapse;border:none'>
 <thead>
  <tr style='height:14.05pt'>
   <td valign=bottom style='border:solid windowtext 1.0pt;padding:0in 0in 6.85pt 0in;
   height:14.05pt'>
   <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>TASK</b></p>
   </td>
   <td valign=bottom style='border:solid windowtext 1.0pt;border-left:none;
   padding:0in 0in 6.85pt 0in;height:14.05pt'>
   <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>PURPOSE</b></p>
   </td>
   <td valign=bottom style='border:solid windowtext 1.0pt;border-left:none;
   padding:0in 0in 6.85pt 0in;height:14.05pt'>
   <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>KEY
   DATASETS</b></p>
   </td>
   <td valign=bottom style='border:solid windowtext 1.0pt;border-left:none;
   padding:0in 0in 6.85pt 0in;height:14.05pt'>
   <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>DATASET
   FEATURES</b></p>
   </td>
   <td valign=bottom style='border:solid windowtext 1.0pt;border-left:none;
   padding:0in 0in 6.85pt 0in;height:14.05pt'>
   <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>APPLICATIONS</b></p>
   </td>
  </tr>
 </thead>
 <tr style='height:44.25pt'>
  <td valign=bottom style='border:solid windowtext 1.0pt;border-top:none;
  padding:6.85pt 0in 6.85pt 0in;height:44.25pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Image
  Retrieval (IR)</b></p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  44.25pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Retrieve
  relevant images from massive repositories using textual queries.</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  44.25pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Custom
  large-scale repositories</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  44.25pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Built from
  satellite data (e.g., Sentinel-2, Gaofen) for multimodal search.</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  44.25pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Disaster
  response, historical analysis.</p>
  </td>
 </tr>
 <tr style='height:44.25pt'>
  <td valign=bottom style='border:solid windowtext 1.0pt;border-top:none;
  padding:6.85pt 0in 6.85pt 0in;height:44.25pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Visual
  Question Answering (VQA)</b></p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  44.25pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Answer
  open-ended questions about RS imagery (e.g., &quot;Is there flooding?&quot;).</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  44.25pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>RSVQA-LR,
  RSVQA-HR</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  44.25pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>RSVQA-LR:
  Low-res; RSVQA-HR: High-res; both include diverse Q&amp;A pairs.</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  44.25pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Real-time
  monitoring, educational tools.</p>
  </td>
 </tr>
 <tr style='height:43.25pt'>
  <td valign=bottom style='border:solid windowtext 1.0pt;border-top:none;
  padding:6.85pt 0in 6.85pt 0in;height:43.25pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Image
  Captioning (IC)</b></p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  43.25pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Generate
  descriptive text summaries of RS imagery.</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  43.25pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>RSICD,
  NWPU-Captions, CapERA</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  43.25pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>RSICD: 10k+
  images with 5 captions each; CapERA: UAV videos with text descriptions.</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  43.25pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Automated
  reporting, accessibility tools.</p>
  </td>
 </tr>
 <tr style='height:44.25pt'>
  <td valign=bottom style='border:solid windowtext 1.0pt;border-top:none;
  padding:6.85pt 0in 6.85pt 0in;height:44.25pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Visual
  Grounding (VG)</b></p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  44.25pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Localize
  objects described in text (e.g., &quot;Find the red-roofed building&quot;).</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  44.25pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>RSVGD (DIOR-based)</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  44.25pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>192k+
  instances with text queries; addresses scale/clutter challenges.</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  44.25pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Military
  targeting, infrastructure inspection.</p>
  </td>
 </tr>
 <tr style='height:44.25pt'>
  <td valign=bottom style='border:solid windowtext 1.0pt;border-top:none;
  padding:6.85pt 0in 6.85pt 0in;height:44.25pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Remote
  Sensing Image Change Captioning (RSICC)</b></p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  44.25pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Describe
  land-cover changes in multitemporal images.</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  44.25pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>LEVIR-CC</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  44.25pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>10k image pairs
  with 50k sentences detailing changes.</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  44.25pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Climate
  change analysis, post-disaster assessment.</p>
  </td>
 </tr>
 <tr style='height:43.25pt'>
  <td valign=bottom style='border:solid windowtext 1.0pt;border-top:none;
  padding:6.85pt 0in 6.85pt 0in;height:43.25pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Referring
  to Remote Sensing Image Segmentation (RRSIS)</b></p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  43.25pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Segment
  objects based on textual prompts (e.g., &quot;Mask flooded areas&quot;).</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  43.25pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>RefSegRS,
  RRSIS-D</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  43.25pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>RefSegRS: pixel-level
  masks; RRSIS-D: 17k+ image-caption-mask triplets.</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  43.25pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Precision
  agriculture, hazard mapping.</p>
  </td>
 </tr>
</table>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>&nbsp;</b></p>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Key
Strengths of VLMs:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Multimodal
     Flexibility: Perform tasks like captioning, VQA, and segmentation in a
     unified framework. </li>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Human-Like
     Reasoning: Answer complex queries (e.g., &quot;Count vehicles near the
     highway exit&quot;). </li>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Generalization:
     Adapt to novel tasks (e.g., RSICC) with minimal fine-tuning.</li>
</ul>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>&nbsp;</b></p>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Comparison
of Pure Visual vs. Vision–Language Tasks</b></p>

<table class=MsoNormalTable border=1 cellspacing=0 cellpadding=0 width=609
 style='width:456.6pt;border-collapse:collapse;border:none'>
 <thead>
  <tr style='height:13.85pt'>
   <td width=132 valign=bottom style='width:99.0pt;border:solid windowtext 1.0pt;
   padding:0in 0in 6.85pt 0in;height:13.85pt'>
   <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>ASPECT</b></p>
   </td>
   <td width=192 valign=bottom style='width:143.9pt;border:solid windowtext 1.0pt;
   border-left:none;padding:0in 0in 6.85pt 0in;height:13.85pt'>
   <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>PURE
   VISUAL TASKS</b></p>
   </td>
   <td valign=bottom style='border:solid windowtext 1.0pt;border-left:none;
   padding:0in 0in 6.85pt 0in;height:13.85pt'>
   <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>VISION–LANGUAGE
   TASKS</b></p>
   </td>
  </tr>
 </thead>
 <tr style='height:14.85pt'>
  <td width=132 valign=bottom style='width:99.0pt;border:solid windowtext 1.0pt;
  border-top:none;padding:6.85pt 0in 6.85pt 0in;height:14.85pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Input</b></p>
  </td>
  <td width=192 valign=bottom style='width:143.9pt;border-top:none;border-left:
  none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:6.85pt 0in 6.85pt 0in;height:14.85pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Images only.</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  14.85pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Images + Text
  (queries/captions).</p>
  </td>
 </tr>
 <tr style='height:28.7pt'>
  <td width=132 valign=bottom style='width:99.0pt;border:solid windowtext 1.0pt;
  border-top:none;padding:6.85pt 0in 6.85pt 0in;height:28.7pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Focus</b></p>
  </td>
  <td width=192 valign=bottom style='width:143.9pt;border-top:none;border-left:
  none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:6.85pt 0in 6.85pt 0in;height:28.7pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Feature
  extraction (spatial/spectral patterns).</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  28.7pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Multimodal interaction
  (language-guided analysis).</p>
  </td>
 </tr>
 <tr style='height:13.85pt'>
  <td width=132 valign=bottom style='width:99.0pt;border:solid windowtext 1.0pt;
  border-top:none;padding:6.85pt 0in 6.85pt 0in;height:13.85pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Complexity</b></p>
  </td>
  <td width=192 valign=bottom style='width:143.9pt;border-top:none;border-left:
  none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:6.85pt 0in 6.85pt 0in;height:13.85pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Narrowly
  scoped (single-task models).</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  13.85pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Broadly
  scoped (multi-task, open-ended).</p>
  </td>
 </tr>
 <tr style='height:28.7pt'>
  <td width=132 valign=bottom style='width:99.0pt;border:solid windowtext 1.0pt;
  border-top:none;padding:6.85pt 0in 6.85pt 0in;height:28.7pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Dataset
  Requirements</b></p>
  </td>
  <td width=192 valign=bottom style='width:143.9pt;border-top:none;border-left:
  none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:6.85pt 0in 6.85pt 0in;height:28.7pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Large labeled
  image datasets.</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  28.7pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Image-text
  pairs or annotated Q&amp;A datasets.</p>
  </td>
 </tr>
 <tr style='height:28.7pt'>
  <td width=132 valign=bottom style='width:99.0pt;border:solid windowtext 1.0pt;
  border-top:none;padding:6.85pt 0in 6.85pt 0in;height:28.7pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Example
  Use Case</b></p>
  </td>
  <td width=192 valign=bottom style='width:143.9pt;border-top:none;border-left:
  none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:6.85pt 0in 6.85pt 0in;height:28.7pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Classify
  urban vs. rural areas.</b></p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  28.7pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Answer:
  &quot;How many buildings were constructed after 2020 in this region?&quot;</b></p>
  </td>
 </tr>
</table>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>&nbsp;</b></p>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Impact of
VLMs in Remote Sensing</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Advantages
     Over Traditional Models:</b></li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Multitasking</b>:
      Replace siloed models with unified frameworks (e.g., LLaVA handles OD,
      IC, and VQA). </li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Scalability</b>:
      Leverage pretrained LLMs (e.g., GPT-4, Vicuna) for zero-shot adaptation. </li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Interpretability</b>:
      Generate human-readable explanations (e.g., change descriptions in
      RSICC).</li>
 </ul>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Challenges:</b></li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Data
      Bias: </b>Auto-annotated datasets may inherit biases from LLMs.<b> </b></li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Computational
      Cost: </b>Training VLMs requires significant GPU resources.</li>
 </ul>
</ul>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>&nbsp;</b></p>

<h2><a name="_Toc193726736">RECENT ADVANCES</a></h2>

<h3><a name="_Toc193726737">1. Contrastive VLMs</a></h3>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Objective: </b>Align
image and text features in a shared embedding space.</p>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b><img
border=0 width=468 height=151
src="Advancements%20in%20Vision–Language%20Models.fld/image010.png"
alt="A diagram of a conversation&#10;&#10;AI-generated content may be incorrect."></b><br>
<b>Key Models:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>RemoteCLIP:
     </b>Combines existing datasets and fine-tunes CLIP for remote sensing,
     enabling multi-task capabilities (zero-shot classification, retrieval).<b>
     </b></li>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>GeoRSCLIP:
     </b>Uses 5M image-text pairs with CLIP fine-tuning for geospatial
     generalization.<b> </b></li>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>ChangeCLIP:
     </b>Uses a Differentiable Feature Calculation (DFC) layer for change
     detection tasks.</li>
</ul>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Methods:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Training</b>:
     Contrastive loss (InfoNCE) to maximize similarity between matched
     image-text pairs. </li>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Data</b>:
     Combines existing datasets (e.g., AID, DIOR) or auto-annotated data for
     scalability.</li>
</ul>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Pros:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Efficient
     pre-training for downstream tasks. </li>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Strong
     generalization to unseen classes (zero/few-shot).</li>
</ul>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Cons:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Limited to
     simple alignment tasks (no complex reasoning). </li>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Requires
     domain adaptation for geospatial nuances.</li>
</ul>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>&nbsp;</b></p>

<h3><a name="_Toc193726738">2. Advanced Conversational VLMs</a></h3>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Objective: </b>Integrate
LLMs (e.g., Vicuna, LLaMA) with vision encoders for multimodal reasoning.</p>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><img border=0
width=468 height=434 id="Picture 1"
src="Advancements%20in%20Vision–Language%20Models.fld/image011.png"
alt="A diagram of a process&#10;&#10;AI-generated content may be incorrect."><br>
<b>Key Models:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>SkyEyeGPT:
     </b>Uses a CLIP-based encoder and Vicuna LLM, excelling in VQA and
     captioning. </li>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>RS-LLaVA: </b>Combines
     LLaVA with domain-specific image encoders for geospatial semantics. </li>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>EarthGPT: </b>Integrates
     SAR/infrared imagery with ViT for multimodal analysis.</li>
</ul>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Methods:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Image
     Encoders</b>: CLIP variants, EVA-CLIP, or hybrid ViT-CNN architectures. </li>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Alignment</b>:
     MLP projection layers or learnable query embeddings (e.g., Q-Former). </li>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Training</b>:
     Instruction tuning on remote sensing-specific datasets (e.g., RRSIS-D).</li>
</ul>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Pros:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Handle
     complex tasks (VQA, change captioning, segmentation). </li>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Support
     region-specific queries and human-like reasoning.</li>
</ul>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Cons:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>High
     computational demands (large LLMs). </li>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Requires
     extensive fine-tuning for domain alignment.</li>
</ul>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>&nbsp;</b></p>

<h3><a name="_Toc193726739">3. Specialized Models</a></h3>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Objective: </b>Address
domain-specific challenges beyond general-purpose VLMs.<br>
<b>Key Models:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>SpectralGPT</b>:
     Trained on spectral data (1M images) for classification and change
     detection. </li>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>CPSeg</b>:
     Uses language prompts for flood segmentation. </li>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>GeoCLIP</b>:
     Aligns satellite images with GPS coordinates for geo-localization<b>. </b></li>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>TEMO: </b>Enhances
     few-shot object detection via text-visual fusion.</li>
</ul>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Methods:</p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Adaptations</b>:
     Spatial-spectral tokenization (SpectralGPT), hash-based retrieval
     (SHRNet). </li>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Data</b>:
     Leverages specialized datasets (e.g., DIOR for few-shot detection).</li>
</ul>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Pros:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Optimized for
     niche tasks (e.g., spectral analysis, geo-localization). </li>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Efficient in
     resource-constrained scenarios (e.g., TEMO for few-shot learning).</li>
</ul>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Cons:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Lack
     generalizability to broader tasks. </li>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Dependent on
     large annotated datasets (e.g., EuroSAT for SpectralGPT).</li>
</ul>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>&nbsp;</b></p>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Performance
Comparison</b></p>

<table class=MsoNormalTable border=1 cellspacing=0 cellpadding=0 width=645
 style='width:483.65pt;border-collapse:collapse;border:none'>
 <thead>
  <tr style='height:14.55pt'>
   <td valign=bottom style='border:solid windowtext 1.0pt;padding:0in 0in 6.85pt 0in;
   height:14.55pt'>
   <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>MODEL
   TYPE</b></p>
   </td>
   <td valign=bottom style='border:solid windowtext 1.0pt;border-left:none;
   padding:0in 0in 6.85pt 0in;height:14.55pt'>
   <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>BEST
   PERFORMANCE</b></p>
   </td>
   <td valign=bottom style='border:solid windowtext 1.0pt;border-left:none;
   padding:0in 0in 6.85pt 0in;height:14.55pt'>
   <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>STRENGTHS</b></p>
   </td>
   <td valign=bottom style='border:solid windowtext 1.0pt;border-left:none;
   padding:0in 0in 6.85pt 0in;height:14.55pt'>
   <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>WEAKNESSES</b></p>
   </td>
  </tr>
 </thead>
 <tr style='height:30.15pt'>
  <td valign=bottom style='border:solid windowtext 1.0pt;border-top:none;
  padding:6.85pt 0in 6.85pt 0in;height:30.15pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Contrastive
  VLMs</b></p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  30.15pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>95.1% SC
  (AID), 89.3% IR (WHU-RS19)</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  30.15pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Zero-shot
  adaptability, efficient pre-training.</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  30.15pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Limited to
  retrieval/classification.</p>
  </td>
 </tr>
 <tr style='height:30.15pt'>
  <td valign=bottom style='border:solid windowtext 1.0pt;border-top:none;
  padding:6.85pt 0in 6.85pt 0in;height:30.15pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Conversational
  VLMs</b></p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  30.15pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>79.6% VQA
  (RSVQA-HR), 98.2% IC (EuroSAT)</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  30.15pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Multi-task flexibility,
  human-like reasoning.</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  30.15pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>High
  computational cost, complex fine-tuning.</p>
  </td>
 </tr>
 <tr style='height:30.15pt'>
  <td valign=bottom style='border:solid windowtext 1.0pt;border-top:none;
  padding:6.85pt 0in 6.85pt 0in;height:30.15pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Specialized
  Models</b></p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  30.15pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>99.21% SC
  (SpectralGPT), 75.1% OD (TEMO)</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  30.15pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Domain-specific
  accuracy (e.g., floods, spectra).</p>
  </td>
  <td valign=bottom style='border-top:none;border-left:none;border-bottom:solid windowtext 1.0pt;
  border-right:solid windowtext 1.0pt;padding:6.85pt 0in 6.85pt 0in;height:
  30.15pt'>
  <p class=MsoNormal style='margin-bottom:0in;line-height:normal'>Narrow
  applicability, data dependency.</p>
  </td>
 </tr>
</table>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Key Findings:</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Conversational
     VLMs surpass contrastive models in tasks requiring reasoning (e.g., VQA,
     IC). </li>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Contrastive
     VLMs excel in data efficiency but lack nuanced interaction. </li>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'>Specialized models
     are critical for niche applications but do not generalize.</li>
</ul>

<h1>&nbsp;</h1>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Conclusion</b></p>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Contrastive
     VLMs:</b> Ideal for tasks needing robust feature alignment (e.g.,
     retrieval). </li>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Conversational
     VLMs:</b> Preferred for complex, open-ended tasks (e.g., multisensor
     analysis) despite higher resource demands. </li>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Specialized
     Models:</b> Fill gaps in spectral, geo-localization, and few-shot tasks.<br>
     Future Direction: Unified benchmarks and hybrid approaches (e.g.,
     conversational + spectral models) will drive next-gen VLMs in remote
     sensing.</li>
</ul>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>&nbsp;</b></p>

<h2><a name="_Toc193726740">GAPS AND FUTURE WORK IN VISION–LANGUAGE MODELS
(VLMS) FOR REMOTE SENSING</a></h2>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>&nbsp;</b></p>

<h3><a name="_Toc193726741">Identified Gaps (Current Shortcomings)</a></h3>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Regression
     Tasks: </b></li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Issue: </b>Tokenization
      of numerical values (e.g., &quot;100&quot; split into &quot;1&quot; and
      &quot;00&quot;) leads to precision loss in regression tasks like
      Above-Ground Biomass (AGB) estimation. </li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Root
      Cause: </b>Text-based tokenizers fail to capture numerical relationships,
      limiting VLMs in tasks requiring continuous value predictions.</li>
 </ul>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Structural
     Characteristics of Remote Sensing (RS) Data: </b></li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Issue:
      Existing </b>VLMs rely on RGB-specific architectures and lack specialized
      models for multispectral (e.g., HSI) or radar (e.g., SAR) data.<b> </b></li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Root
      Cause: </b>Pre-training frameworks are inherited from general computer
      vision, compromising performance on SAR/HSI modalities.</li>
 </ul>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Multimodal
     Output Limitation: </b></li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Issue: </b>VLMs
      produce text-only outputs, limiting their utility in dense prediction
      tasks (e.g., segmentation, 3D modeling).<b> </b></li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Root
      Cause: </b>Current architectures prioritize language generation over
      joint text-visual outputs (e.g., images or masks).</li>
 </ul>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Temporal
     Data Handling: </b></li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Issue: </b>VLMs
      focus on static imagery, neglecting temporal dynamics critical for
      climate monitoring and land-use prediction.<b> </b></li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Root
      Cause: </b>Lack of sequence modeling frameworks to encode multitemporal
      remote sensing data.</li>
 </ul>
</ol>

<h3><a name="_Toc193726742">Future Work Directions</a></h3>

<ol style='margin-top:0in' start=1 type=1>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Enhancing
     Regression Capabilities: </b></li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Specialized
      Tokenizers:</b> Design tokenizers to preserve numerical precision (e.g.,
      treating &quot;100&quot; as a single token). </li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Regression
      Heads:</b> Integrate task-specific heads (e.g., MLP-mixers) to bypass
      text token limitations. </li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Mixture
      of Experts (MOE):</b> Use gating mechanisms to dynamically activate
      regression modules for multi-task learning. </li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Example: </b>REO-VLM
      employs hybrid visual-language embeddings for accurate AGB estimation.</li>
 </ul>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Domain-Specific
     Architectures for RS Data: </b></li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Multispectral/SAR
      Encoders</b>: Develop feature extractors tailored to hyperspectral or
      radar data (e.g., pseudo-RGB conversion for SAR). </li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Multimodal
      Datasets</b>: Expand datasets to include SAR, HSI, LiDAR, and text pairs
      for robust cross-modal alignment.</li>
 </ul>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Multimodal
     Output Generation: </b></li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Beyond
      Text</b>: Enable VLMs to output images, videos, or 3D models (e.g., Emu3
      uses next-token prediction for image generation). </li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Hybrid
      Models: </b>Integrate VLMs with task-specific heads (e.g., segmentation
      masks) using LLMs as &quot;controllers<b>.&quot;</b></li>
 </ul>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Temporal-Spatial
     Modeling: </b></li>
 <ul style='margin-top:0in' type=circle>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Sequence
      Encoding</b>: Treat time-series RS data as sequential inputs for
      transformers to capture trends (e.g., deforestation). </li>
  <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Predictive
      Analytics</b>: Train VLMs on historical data to forecast environmental
      changes or disaster impacts.</li>
 </ul>
</ol>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>&nbsp;</b></p>

<h3><a name="_Toc193726743">Additional Considerations</a></h3>

<ul style='margin-top:0in' type=disc>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Benchmarking</b>:
     Establish unified evaluation standards for cross-modal tasks (e.g., VQA,
     change captioning). </li>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Sustainability</b>:
     Optimize energy-efficient training methods for large-scale VLMs to reduce
     computational costs. </li>
 <li class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>Ethical</b>
     <b>AI</b>: Address biases in auto-annotated datasets to ensure fairness in
     applications like urban planning.</li>
</ul>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>&nbsp;</b></p>

<h3><a name="_Toc193726744">Conclusion</a></h3>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'>While VLMs have
transformed remote sensing analysis, addressing gaps in regression,
multimodality, domain adaptation, and temporal modeling will unlock their full
potential. Future efforts should prioritize specialized architectures,
temporal-spatial integration, and ethical, scalable solutions to bridge the gap
between AI innovation and real-world geospatial challenges.</p>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b>&nbsp;</b></p>

</div>

</body>

</html>
